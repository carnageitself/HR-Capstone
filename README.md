<p align="center">
  <img src="public/Northeastern_Banner.png" alt="Northeastern University" width="100%" height="80" style="object-fit:cover"/>
</p>

# ğŸ† HR Recognition Analytics Dashboard

<p align="center">A full-stack data science project that transforms raw employee recognition data into actionable HR intelligence. Built in partnership with <strong>Workhuman</strong> as part of a Master's Capstone at <strong>Northeastern University</strong>.</p>

## ğŸ“‹ Table of Contents

- [Project Overview](#-project-overview)
- [Architecture](#-architecture)
- [Project Structure](#-project-structure)
- [Data Schema](#-data-schema)
- [Prerequisites](#-prerequisites)
- [Quick Start](#-quick-start)
- [Running the Analysis Pipeline](#-running-the-analysis-pipeline)
- [Running the Dashboard](#-running-the-dashboard)
- [Pipeline Deep Dive](#-pipeline-deep-dive)
- [Dashboard Features](#-dashboard-features)
- [Configuration Reference](#-configuration-reference)
- [Troubleshooting](#-troubleshooting)

---

## ğŸ¯ Project Overview

This project processes employee recognition award data through a two-stage pipeline:

1. **Taxonomy Pipeline** â€” Uses LLMs (Claude / Gemini) and a local SLM (Ollama/Llama3) to automatically discover and refine a category taxonomy from recognition messages using grounded-theory methodology.

2. **Visualization Dashboard** â€” A Next.js dashboard that renders HR insights including recognition network graphs, culture health scorecards, department analytics, and message-level intelligence.

### Key Capabilities

- Automatically categorizes recognition messages into meaningful HR themes
- Surfaces recognition coverage gaps across departments and seniority levels
- Visualizes recognition networks to identify influence patterns
- Tracks culture health indicators per department
- Generates word cloud and thematic analysis of award language

---

## ğŸ“¸ Screenshots

### Overview â€” Workforce Health & Recognition Reach

> KPI cards, recognition coverage donut charts, monthly trend line, and seniority breakdown.

![Overview Tab](/public/screenshot_overview.png)

---

### People â€” Employee Recognition Profiles

> Per-employee table with engagement scores, skills, days-since-last-recognition, and a detailed side panel with full recognition history.

![People Tab](/public/screenshot_people.png)

---

### Departments â€” Coverage & Participation by Team

> Per-department cards showing recognition coverage %, peer participation %, average awards, and unrecognized headcount.

![Departments Tab](/public/screenshot_departments.png)

---

### HR Intelligence â€” Seasonality Heatmap & Advanced Analytics

> Advanced intelligence tools: Invisible Contributors, Momentum Tracker, Influence Map, Equity Lens, Skill Gap Radar, Seasonality Heatmap, Org Connectors, and Value Equity Audit.

![HR Intelligence Tab](/public/screenshot_hr_intelligence.png)

---

## ğŸ— Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATA SOURCES (CSV)                    â”‚
â”‚  awards Â· employees Â· departments Â· skills Â· companies   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               TAXONOMY PIPELINE  (Python)                â”‚
â”‚                                                          â”‚
â”‚  Phase 1: Claude / Gemini  â†’  Seed Taxonomy              â”‚
â”‚  Phase 2: Ollama / Llama3  â†’  Bulk Classification        â”‚
â”‚  Phase 3: Claude / Gemini  â†’  Finalize & Refine          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚  outputs annotated CSVs + JSON
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            NEXT.JS DASHBOARD  (TypeScript)               â”‚
â”‚                                                          â”‚
â”‚  Overview Â· Departments Â· People Â· Recognition Activity  â”‚
â”‚  HR Intelligence (Network Â· Culture Health Â· Messages)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Project Structure

```
hr-analytics/                       # Project root (Next.js app)
â”‚
â”œâ”€â”€ app/                            # Next.js App Router
â”‚   â”œâ”€â”€ favicon.ico
â”‚   â”œâ”€â”€ globals.css                 # Global styles
â”‚   â”œâ”€â”€ layout.tsx                  # Root layout
â”‚   â”œâ”€â”€ page.tsx                    # Entry page (redirects to dashboard)
â”‚   â”œâ”€â”€ components/                 # Shared React components
â”‚   â””â”€â”€ constants/                  # App-wide constants
â”‚
â”œâ”€â”€ data/                           # All CSV data files
â”‚   â”œâ”€â”€ award_categories.csv
â”‚   â”œâ”€â”€ awards.csv
â”‚   â”œâ”€â”€ awards_enriched.csv
â”‚   â”œâ”€â”€ companies.csv
â”‚   â”œâ”€â”€ departments.csv
â”‚   â”œâ”€â”€ employee_skills.csv
â”‚   â”œâ”€â”€ employees.csv
â”‚   â””â”€â”€ skills.csv
â”‚
â”œâ”€â”€ lib/                            # Server-side utilities
â”‚   â”œâ”€â”€ loadDashboardData.ts        # CSV â†’ typed data aggregations
â”‚   â””â”€â”€ parseCSV.ts                 # Custom CSV parser
â”‚
â”œâ”€â”€ public/                         # Static assets
â”‚   â”œâ”€â”€ Northeastern Logo.png
â”‚   â”œâ”€â”€ screenshot_overview.png
â”‚   â”œâ”€â”€ screenshot_people.png
â”‚   â”œâ”€â”€ screenshot_departments.png
â”‚   â”œâ”€â”€ screenshot_hr_intelligence.png
â”‚   â”œâ”€â”€ file.svg
â”‚   â”œâ”€â”€ globe.svg
â”‚   â”œâ”€â”€ next.svg
â”‚   â”œâ”€â”€ vercel.svg
â”‚   â””â”€â”€ window.svg
â”‚
â”œâ”€â”€ scripts/                        # Utility / data generation scripts
â”‚
â”œâ”€â”€ taxonomy_pipeline/              # Python taxonomy pipeline
â”‚   â”œâ”€â”€ config.py                   # All configuration & API keys
â”‚   â”œâ”€â”€ defaults.py                 # Fallback taxonomy defaults
â”‚   â”œâ”€â”€ utils.py                    # Shared helpers (LLM clients, CSV loading)
â”‚   â”œâ”€â”€ phase_1_seed.py             # LLM discovers initial taxonomy
â”‚   â”œâ”€â”€ phase_2_bulk.py             # Local SLM bulk-classifies messages
â”‚   â”œâ”€â”€ phase_3_finalize.py         # LLM refines & finalizes taxonomy
â”‚   â”œâ”€â”€ run_pipeline.py             # Main pipeline entry point
â”‚   â””â”€â”€ run_comparison.py           # Compare multiple provider configs
â”‚
â”œâ”€â”€ .env.local                      # API keys (never commit this)
â”œâ”€â”€ .gitignore
â”œâ”€â”€ eslint.config.mjs
â”œâ”€â”€ mockup_awards.csv               # Mockup data for pipeline testing
â”œâ”€â”€ next.config.ts                  # Next.js configuration
â”œâ”€â”€ next-env.d.ts
â”œâ”€â”€ package.json
â”œâ”€â”€ package-lock.json
â”œâ”€â”€ postcss.config.mjs
â”œâ”€â”€ tsconfig.json
â””â”€â”€ README.md
```

---

## ğŸ—‚ Data Schema

| File | Rows | Description |
|------|------|-------------|
| `awards.csv` | 1,000+ | Core recognition records with messages, category tags, and monetary values |
| `awards_enriched.csv` | 1,000 | Awards with full recipient/nominator detail, taxonomy annotations |
| `mockup_awards_enriched.csv` | 2,000 | Extended mockup dataset for testing |
| `employees.csv` | 50â€“60 | Employee profiles, job titles, seniority, department assignments |
| `departments.csv` | 10â€“18 | Department info, headcounts, manager IDs |
| `companies.csv` | 3 | Parent company metadata |
| `skills.csv` | 10â€“36 | Skill definitions and categories |
| `employee_skills.csv` | 183 | Employee-to-skill mappings with proficiency levels |
| `categories.csv` | 6 | Recognition category definitions with colors |
| `subcategories.csv` | 18 | Subcategory breakdowns per category |

---

## âœ… Prerequisites

### For the Dashboard (Frontend)

| Requirement | Version |
|-------------|---------|
| Node.js | 18.x or higher |
| npm | 9.x or higher |

### For the Pipeline (Backend)

| Requirement | Version |
|-------------|---------|
| Python | 3.9 or higher |
| pip | latest |
| Ollama | latest (for Phase 2) |

### API Keys (at least one required for pipeline)

| Provider | Where to get it | Notes |
|----------|----------------|-------|
| Anthropic (Claude) | https://console.anthropic.com/settings/keys | Paid, highest quality |
| Google (Gemini) | https://aistudio.google.com/apikey | Free tier available |

---

## ğŸš€ Quick Start

### 1. Clone and navigate to the project

```bash
git clone <your-repo-url>
cd <project-root>
```

### 2. Set up your environment variables

Create a `.env.local` file in the project root (it's already in `.gitignore`):

```env
# .env.local
ANTHROPIC_API_KEY=sk-ant-...   # Optional: Claude API key
GOOGLE_API_KEY=AIza...          # Optional: Gemini API key
```

> At least one key is required to run the taxonomy pipeline. The dashboard works without any keys.

---

## ğŸ“Š Running the Dashboard

The dashboard is a Next.js app that reads CSV files directly from the `data/` folder.

### Step 1 â€” Install dependencies

```bash
npm install
```

### Step 2 â€” Make sure data files are in place

The dashboard reads from the `data/` folder at the project root. Your CSV files should already be there. If not:

```bash
cp /path/to/your/csvs/*.csv data/
```

### Step 3 â€” Start the development server

```bash
npm run dev
```

Open [http://localhost:3000/dashboard](http://localhost:3000/dashboard) in your browser.

### Build for production

```bash
npm run build
npm start
```

---

## âš™ï¸ Running the Analysis Pipeline

The pipeline categorizes recognition messages using a 3-phase LLM approach.

### Step 1 â€” Install Python dependencies

```bash
cd taxonomy_pipeline
pip install -r requirements.txt
```

If you don't have a `requirements.txt`, install the core packages:

```bash
pip install anthropic google-generativeai pandas python-dotenv requests
```

### Step 2 â€” (Optional) Install Ollama for Phase 2

Phase 2 uses a local Llama3 model for bulk processing â€” this avoids API costs.

1. Download and install Ollama from https://ollama.ai
2. Pull the model:

```bash
ollama pull llama3:8b
```

3. Start the Ollama server (usually starts automatically):

```bash
ollama serve
```

### Step 3 â€” Run the full pipeline

```bash
cd taxonomy_pipeline
python run_pipeline.py
```

### Pipeline options

```bash
# Run only a specific phase
python run_pipeline.py --phase 1
python run_pipeline.py --phase 2
python run_pipeline.py --phase 3

# Skip Phase 2 (if Ollama is not available)
python run_pipeline.py --skip-phase2

# Force a specific LLM provider
python run_pipeline.py --provider claude
python run_pipeline.py --provider gemini

# Save results to a named run folder (for comparisons)
python run_pipeline.py --run-name my_experiment

# Compare multiple provider configurations
python run_comparison.py
```

### Phase 2 resume / crash recovery

Phase 2 automatically saves checkpoints every 20 batches. If it crashes, just re-run â€” it will resume from the last checkpoint automatically.

---

## ğŸ”¬ Pipeline Deep Dive

### Phase 1 â€” Taxonomy Discovery

A sample of 100 recognition messages is sent to Claude or Gemini. The LLM applies grounded theory to surface 6â€“8 meaningful categories with subcategories and descriptions. Output: `outputs/phase_1_taxonomy.json`.

### Phase 2 â€” Bulk Classification

All messages are classified in batches of 5 using a local Llama3 model via Ollama. This phase runs entirely offline with no API costs. It also surfaces candidate new categories that appear frequently but weren't in the Phase 1 taxonomy. Output: `outputs/phase_2_classifications.json`.

### Phase 3 â€” Taxonomy Finalization

Claude or Gemini reviews the Phase 1 taxonomy alongside Phase 2's candidate suggestions and produces a refined final taxonomy (max 8 categories, 4 subcategories each). Output: `outputs/phase_3_final_taxonomy.json`.

---

## ğŸ–¥ Dashboard Features

| Tab | What it shows |
|-----|--------------|
| **Overview** | KPI cards, monthly trend line, recognition coverage, category breakdown |
| **People** | Per-employee recognition summary, days-since-last-recognition, seniority heatmap |
| **Departments** | Department-level coverage, nomination participation rates, cross-department recognition |
| **Recognition Activity** | Award volume over time, top nominators, category distribution |
| **HR Intelligence** | Recognition Network Graph, Culture Health Scorecard, Message Word Cloud & Themes |

### HR Intelligence Suite highlights

- **Recognition Network Graph** â€” Force-directed graph showing who recognizes whom, surfacing informal influence hubs
- **Culture Health Scorecard** â€” Per-department scoring on consistency, participation, and cross-team recognition
- **Message Insights** â€” Word frequency cloud and thematic breakdown of award language

---

## âš™ï¸ Configuration Reference

All pipeline settings live in `pipeline/config.py`:

| Setting | Default | Description |
|---------|---------|-------------|
| `LLM_PROVIDER_PRIORITY` | `["claude", "gemini"]` | Provider fallback order |
| `P1_SAMPLE_SIZE` | `100` | Messages sampled for taxonomy discovery |
| `P2_BATCH_SIZE` | `5` | Messages per Ollama batch |
| `P2_MODEL` | `"llama3:8b"` | Local Ollama model tag |
| `P2_CHECKPOINT_EVERY` | `20` | Batches between checkpoint saves |
| `P3_MAX_MAIN_CATEGORIES` | `8` | Max final categories |
| `P3_MAX_SUBCATEGORIES` | `4` | Max subcategories per category |
| `LOG_LEVEL` | `"INFO"` | Logging verbosity |

---

## ğŸ›  Troubleshooting

**Dashboard shows no data**
â†’ Make sure CSV files are present in `employee-dashboard/data/` and are not empty.

**Pipeline: "No LLM API key found"**
â†’ Create a `.env` file in the project root with at least one of `ANTHROPIC_API_KEY` or `GOOGLE_API_KEY`.

**Pipeline Phase 2: "Ollama not running"**
â†’ Run `ollama serve` in a separate terminal, or use `--skip-phase2` to bypass it.

**Pipeline Phase 2: "model not found"**
â†’ Run `ollama pull llama3:8b` to download the model first.

**Dashboard: hydration or import errors**
â†’ Run `npm install` again and make sure you're on Node.js 18+.

**Phase 2 crashed mid-run**
â†’ Just re-run `python run_pipeline.py --phase 2` â€” it will resume from the last checkpoint automatically.

---

## ğŸ§‘â€ğŸ’» Development Notes

- The dashboard uses **Next.js Server Components** for data loading â€” all CSV parsing happens server-side with zero client-side fetching.
- The pipeline supports **multi-provider comparison** via `run_comparison.py`, saving separate output folders per run for side-by-side analysis.
- For larger datasets, consider upgrading the Phase 2 model to `qwen2.5:14b` via Ollama for improved classification accuracy.

---

## ğŸ“„ License

This project is part of a Master's Capstone program. All data is synthetic/anonymized for academic purposes.